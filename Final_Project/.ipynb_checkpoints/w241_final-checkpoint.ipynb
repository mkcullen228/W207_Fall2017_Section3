{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type Prediction\n",
    "### Authors: Maura Cullin, Mike Gruzynski\n",
    "#### w207 Final project section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: We choose to do the kaggle competition: Forest Cover Type Prediction (https://www.kaggle.com/c/forest-cover-type-prediction). The data comes from US Forest Service (USFS) Region 2 Resource Information System data and the Independent variables were then derived from data obtained from the US Geological Survey and USFS.\n",
    "\n",
    "This study area includes four wilderness areas located in the Roosevelt National Forest, which is located in northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes (or natural setting) rather than forest management practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mcullen/MyFolder/Documents/UCB_MIDS/W207_AppliedMachineLearning/W207_Fall2017_Section3/Final_Project/final_project\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File /Users/mcullen/MyFolder/Documents/UCB_MIDS/W207_AppliedMachineLearning/W207_Fall2017_Section3/Final_Project/final_project/train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1de943941cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/final_project\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_train_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cover_Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /Users/mcullen/MyFolder/Documents/UCB_MIDS/W207_AppliedMachineLearning/W207_Fall2017_Section3/Final_Project/final_project/train.csv does not exist"
     ]
    }
   ],
   "source": [
    "# load the raw dataset\n",
    "file_path = os.getcwd() + \"/final_project\"\n",
    "file_path = os.getcwd() + \"/data\"\n",
    "print file_path\n",
    "train_df = pd.read_csv(file_path + '/train.csv')\n",
    "df_train_raw = train_df\n",
    "target = train_df['Cover_Type']\n",
    "train_df = train_df.drop('Cover_Type', 1)\n",
    "train_df = train_df.drop('Soil_Type7', 1)\n",
    "train_df = train_df.drop('Soil_Type15', 1)\n",
    "train_df = train_df.drop('Id',1)\n",
    "\n",
    "# print train_df.head()\n",
    "test_df = pd.read_csv(file_path + '/test.csv')\n",
    "# print test_df.head()\n",
    "\n",
    "predictors = train_df.columns\n",
    "l = len(target)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train_df, target, test_size=0.30, random_state=42)\n",
    "print \"Train data size %i X %i\" % (X_train.shape[0], X_train.shape[1])\n",
    "print \"Test data size %i X %i\" % (X_dev.shape[0], X_dev.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 15120 rows in the dataframe, with no missing data. The data is clean and the only filter that will be performed on the dataset is on \"Soil_Type7\",\"Soil_Type15\" because there is only values of 0 for all rows in the training data. We will remove the two columns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up variable types\n",
    "continuous = [\n",
    "            'Elevation', 'Aspect', 'Slope',\n",
    "            'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "            'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'\n",
    "            ]\n",
    "\n",
    "# remove \"Soil_Type7\",\"Soil_Type15\" because their data was only one value (data cannot be seperable and irrelevantc)\n",
    "binary = [\n",
    "        'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
    "        'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "        'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n",
    "        'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
    "        'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "        'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
    "        'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
    "        'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
    "        'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
    "        'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
    "        'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
    "        'Soil_Type40'\n",
    "        ]\n",
    "\n",
    "predictors = continuous + binary\n",
    "\n",
    "# objective variable is a category\n",
    "target = 'Cover_Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix on continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train_raw[continuous].corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "corr_columns = list(corr.columns)\n",
    "corr_comparison_list = []\n",
    "corr_value_list = []\n",
    "for column in corr:\n",
    "    temp_corr_columns = corr_columns   \n",
    "    temp_corr_columns.remove(column)\n",
    "    for index in corr.loc[temp_corr_columns, column].index:\n",
    "        corr_comparison_list.append(str(column) + '_vs_' + str(index))\n",
    "        corr_value_list.append(corr.loc[index, column])\n",
    "        \n",
    "corr_df = pd.DataFrame({\n",
    "    \"corr_comparison\" : corr_comparison_list,\n",
    "    \"corr_value\" : corr_value_list \n",
    "})\n",
    "\n",
    "corr_df = corr_df.iloc[corr_df.corr_value.abs().argsort()][::-1].reset_index()\n",
    "corr_df = corr_df.loc[:, ~corr_df.columns.str.contains('index')]\n",
    "print corr_df.loc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above correlation matrix, we see combination of variables that may cause collinearity issues in the analysis.\n",
    "\n",
    "For example Hillshade_3pm is correlated heavily with Hillshade_9am (corr ~ -0.77).\n",
    "\n",
    "The above dataframe output shows the top 10 (absolute value) correlation values between continuous variables, so we will need to be knowledgeable about having both of the features in the analysis in the top 10 list. The duplicated information may overfit the data and reduce the generalizability of the final analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness information on variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_raw[continuous].skew().sort_values()\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(4, 3) \n",
    "\n",
    "itr = 0\n",
    "for feature in df_train_raw[continuous]:   \n",
    "    if itr == 9:\n",
    "        itr += 1\n",
    "    ax = plt.subplot(gs[itr])\n",
    "    ax.hist(df_train_raw.loc[:, feature])\n",
    "    ax.set_xlabel(\"Histogram Bin\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title('Feature={}, Skew={}'.format(feature, round(df_train_raw.loc[:, feature].skew(), 3), fontsize=12))\n",
    "    \n",
    "    itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above histograms (with skewness value in the title of each plot), we can see that there are some high skews with potential of some log transformations if the math allows it (i.e cant do a log transformations on a zero) so if the column data has a zero we will have to think of doing something else if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize continuous variables mapped to 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 120))\n",
    "gs = gridspec.GridSpec(23, 2) \n",
    "df_column_list = list(df_train_raw[continuous].columns)\n",
    "itr = 0\n",
    "cmap = mpl.colors.ListedColormap(['black','red', 'green', 'blue', 'cyan', 'violet', 'yellow'])\n",
    "black_patch = mpatches.Patch(color='black', label='1')\n",
    "red_patch = mpatches.Patch(color='red', label='2')\n",
    "green_patch = mpatches.Patch(color='green', label='3')\n",
    "blue_patch = mpatches.Patch(color='blue', label='4')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='5')\n",
    "violet_patch = mpatches.Patch(color='violet', label='6')\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='7')\n",
    "\n",
    "\n",
    "for feature in corr:    \n",
    "    temp_columns = df_column_list\n",
    "    index_value = temp_columns.index(feature) + 1\n",
    "    \n",
    "    for sub_feature in temp_columns[index_value:]:\n",
    "        ax = plt.subplot(gs[itr])\n",
    "        ax.scatter(df_train_raw.loc[:, feature], df_train_raw.loc[:, sub_feature], \n",
    "                   c=df_train_raw[target], alpha=0.3, cmap=cmap)\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel(sub_feature)\n",
    "        ax.set_title('{} vs {}'.format(feature, sub_feature, fontsize=12))\n",
    "        ax.legend(handles=[black_patch, red_patch, green_patch, blue_patch, cyan_patch, violet_patch, yellow_patch])\n",
    "        itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After looking at the scatter plots of 2D projection, we can see that there are some very distance regions for target delineation of target cases (the first nine plots - titles shown below):\n",
    "\n",
    "Elevation vs all of its other covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(20, 20))\n",
    "\n",
    "itr_1 = 0\n",
    "itr_2 = 0\n",
    "for feature in continuous:\n",
    "    if itr_2 == 3:\n",
    "        itr_2 = 0\n",
    "        itr_1 += 1\n",
    "    if itr_1 == 3:\n",
    "        itr_2 = 1\n",
    "    df_train_raw[[feature] + [target]].boxplot(by='Cover_Type', ax=axes[itr_1, itr_2])\n",
    "    itr_2 += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above boxplots by forest cover type, the elevation shows very distinct values per category. Elevation is looking like it is a very important feature in determining the target value of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary variable exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df_train_raw[binary + [target]]\n",
    "fig, axes = plt.subplots(nrows=21, ncols=2, figsize=(20, 60))\n",
    "\n",
    "df_binary_columns = []\n",
    "df_dict = {}\n",
    "itr = 0\n",
    "master_df = pd.DataFrame()\n",
    "pic_itr1 = 0\n",
    "pic_itr2 = 0\n",
    "for feature in df_binary:\n",
    "    if 'Cover_Type' not in feature:\n",
    "        df_binary_columns.append(feature)\n",
    "        \n",
    "        if pic_itr2 == 2:\n",
    "            pic_itr2 = 0\n",
    "            pic_itr1 += 1\n",
    "        \n",
    "        itr = 1\n",
    "        for name, group in df_binary.loc[:, [feature] + [target]].groupby('Cover_Type'):\n",
    "            if itr == 1:\n",
    "                df_out = group[feature].value_counts()\n",
    "                df_out.name = 'type_{}'.format(itr)\n",
    "            else:\n",
    "                df_temp = group[feature].value_counts()\n",
    "                df_temp.name = 'type_{}'.format(itr)\n",
    "                df_out = pd.concat([df_out, df_temp], axis = 1)\n",
    "            itr += 1\n",
    "            \n",
    "        \n",
    "        print df_out.plot(kind='bar', title =feature, \n",
    "                          color=['black','red', 'green', 'blue', 'cyan', 'violet', 'yellow'],\n",
    "                          ax=axes[pic_itr1, pic_itr2])\n",
    "        master_df = master_df.append(df_out.rename({0: feature + '_0', 1: feature + '_1'}))\n",
    "        \n",
    "        pic_itr2 += 1\n",
    "        \n",
    "# print master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that a lot of the soil type information is very similar with not a lot of added information to the data analysis. This information along with the continuous data information will help make engineering feature elimination decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Aanlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Choose number of Compenents to Keep\n",
    "num_comp = np.sum(np.cumsum(pca.explained_variance_ratio_) < var_explained)\n",
    "print \"Keep Number of Compenents: %i, explaining %0.2f%% Variance\" %(num_comp, var_explained)\n",
    "\n",
    "# plot \n",
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.title(\"PCA Explained Variance Ratio\")\n",
    "plt.xlabel(\"Number of Fatures\")\n",
    "plt.ylabel(\"Variance Ratio\")\n",
    "plt.show()\n",
    "\n",
    "# Reduce Dataset \n",
    "X_reduced1 = PCA(n_components=num_comp).fit_transform(X_train)\n",
    "print \"Reduced Train set shape: %i X %i\" % (X_reduced1.shape[0],X_reduced1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "X_reduced2 = pca.transform(X_train)[:, :num_comp]\n",
    "\n",
    "# plot \n",
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.title(\"PCA Explained Variance Ratio\")\n",
    "plt.xlabel(\"Number of Fatures\")\n",
    "plt.ylabel(\"Variance Ratio\")\n",
    "plt.show()\n",
    "\n",
    "# Reduce Dataset \n",
    "X_reduced1 = PCA(n_components=num_comp).fit_transform(X_train)\n",
    "print \"Reduced Train set shape: %i X %i\" % (X_reduced1.shape[0],X_reduced1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3\n",
    "Vt = pca.components_[:20, :]\n",
    "M = X_train - X_train.mean()\n",
    "U = np.matmul(M, np.transpose(Vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mapping from PCA to original features \n",
    "comp0 = pd.Series(dict(zip(X_train.columns, Vt[0, :])))\n",
    "comp1 = pd.Series(dict(zip(X_train.columns, Vt[1, :])))\n",
    "col_order = comp0.sort_values().index\n",
    "\n",
    "plt.figure(figsize=(4, 24))\n",
    "comp0[col_order].plot(kind='barh')\n",
    "comp1[col_order].plot(kind='barh', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_features(n_features):\n",
    "    idx = np.argsort(-feature_importance)[:n_features]\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train.iloc[:, idx], y_train)\n",
    "    return lr.score(X_dev.iloc[:, idx], y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = f_regression(X_train, y_train)[0]\n",
    "\n",
    "out = [best_features(i) for i in range(1, n_features)]\n",
    "\n",
    "plt.plot(out)\n",
    "plt.title(\"Prdicted Score with Subset of Features\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"Predicted Linear Regression Score\")\n",
    "plt.show()\n",
    "\n",
    "print np.max(out), np.argmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Look at fetaures used in subset \n",
    "\n",
    "# For example only, if a subset of 15 features used: \n",
    "feats_to_use = 15\n",
    "feats = np.argsort(-feature_importance)[:15]\n",
    "bf_score = best_features(feats_to_use)\n",
    "\n",
    "print \"\\nTop 15 Fetaures in Subset Analysis: Best Score %0.2f%%\" % bf_score\n",
    "print \" Feature%-30s | Importance Value\" % ''\n",
    "for i in range(15):\n",
    "    print \"%-2i %-35s |  %0.3f\" %(i, X_train.columns[feats[i]], feature_importance[feats[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_components(n_features):\n",
    "    pca = PCA(n_components=n_features)\n",
    "    X_transformed = pca.fit_transform(X_train) # choose PCA method from above \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_transformed, y_train)\n",
    "    return lr.score(pca.transform(X_dev), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = [best_components(i) for i in range(1, n_features)]\n",
    "plt.plot(out)\n",
    "plt.title(\"Predicted Linear Regression Score with PCA\")\n",
    "plt.xlabel(\"Number of Principal Compenents\")\n",
    "plt.ylabel(\"Predicted Linear Regression Score\")\n",
    "plt.show()\n",
    "\n",
    "print np.max(out), np.argmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clusts = range(1, 50 )\n",
    "\n",
    "def kmean_rss_score(nclust):\n",
    "    km = Pipeline([('scale', StandardScaler()), ('cluster', KMeans(nclust))])\n",
    "    km.fit(X_train)\n",
    "    rss = -km.score(X_train)\n",
    "    return rss\n",
    "\n",
    "rss = [kmean_rss_score(i) for i in clusts]\n",
    "plt.plot(clusts, rss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Residual Sum of Squares')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_clusters(train_set,num_clusters):\n",
    "    print \"Training Set Dimensions: %i X %i\" % (train_set.shape[0], train_set.shape[1])\n",
    "    # clusters = [1,16]\n",
    "    clusters = np.linspace(1,num_clusters,num_clusters, dtype=int)\n",
    "    position = [(0,0), (0,1), (0,2), (0,3),\n",
    "                (1,0), (1,1), (1,2), (1,3),  \n",
    "                (2,0), (2,1), (2,2), (2,3), \n",
    "                (3,0), (3,1), (3,2), (3,3)]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.suptitle(\"KMeans Clusters over the 2D Projected data\", fontsize=\"x-large\")\n",
    "    \n",
    "    for k in clusters:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km.fit(X_train)\n",
    "        labels = km.labels_\n",
    "        centroids = km.cluster_centers_\n",
    "        dists = km.transform(X_train)\n",
    "        \n",
    "        # Settings for centroid plots\n",
    "        if k == 1: \n",
    "            point, ms = 'k*', 10\n",
    "        else: \n",
    "            point, ms = '*', 7\n",
    "\n",
    "        # Plot Clusters\n",
    "        ax = plt.subplot2grid((4,4), position[k-1])\n",
    "        plt.set_cmap('tab20')  \n",
    "        colors = ['#7e1e9c', '#e50000', '#380835', '#ff81c0', '#0343df', '#15b01a', '#f97306', '#00035b',\n",
    "              '#96f97b', '#6e750e', '#06c2ac', '#95d0fc', '#ff028d', '#516572', '#653700','#ffa756']\n",
    "        for i in range(k):\n",
    "            # find max distances from current centroid for radius\n",
    "            r = np.max(dists[np.where(labels==i)][:,i])\n",
    "            # select only data observations with cluster label == i\n",
    "            ds = train_set[np.where(labels==i)]\n",
    "            # ds = train_set.iloc[np.where(labels==i)]\n",
    "            # plot the data observations\n",
    "            ax.plot(ds[:,0],ds[:,1],'.',color = colors[i])\n",
    "            # plot the centroids and circle with radius to furthest point\n",
    "            lines = ax.plot(centroids[i,0],centroids[i,1], point, ms=ms)\n",
    "            circle = plt.Circle((centroids[i,0],centroids[i,1]), r, color='r', alpha=0.2)\n",
    "            ax.add_artist(circle)\n",
    "        plt.axis('equal')\n",
    "        plt.title(\"KMeans Centroids with \" + str(k) + \" Cluster(s)\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_clusters(X_reduced1, num_clusters = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_clusters(X_reduced2, num_clusters = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters in PCA By true as color\n",
    "# FROM WEEK 9 Notebook\n",
    "# import matplotlib.cm as cm\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=X.shape[1], ncols=X.shape[1], sharex=False, sharey=False, figsize = (16, 16))\n",
    "\n",
    "# cmap = cm.jet\n",
    "# for i, f1 in enumerate(X.columns):\n",
    "#    for j, f2 in enumerate(X.columns):\n",
    "#       if not f1 == f2:\n",
    "#        X.plot(kind='scatter', x=f1, y=f2, c=labels, cmap=cmap, ax=axes[i, j])\n",
    "        \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heirachical Clustering \n",
    "# Could help choose depth of tree (?) \n",
    "for i in range(2, 8):\n",
    "    clusterer = Pipeline(steps=[\n",
    "        ('scale', StandardScaler()),\n",
    "        ('kmeans', KMeans(i))\n",
    "    ])\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "clusters = linkage(X_train, 'ward')\n",
    "_ = dendrogram(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dtree/RF Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Print Classification Summary Scored in Development \n",
    "print \"Accuracy Score on Dev Set: %0.2f%%\" % (dt.score(X_dev,y_dev))\n",
    "preds = dt.predict(X_dev)\n",
    "mse = np.mean((preds - y_dev) ** 2)\n",
    "print 'Mean squared error = {}'.format(mse)\n",
    "print 'R^2 = {}\\n'.format(dt.score(X_train, y_train))\n",
    "\n",
    "print metrics.classification_report(y_dev, preds) # target_names = target_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Feature Importnace in DTREE \n",
    "importance_df = pd.DataFrame({\n",
    "        'feature': predictors,\n",
    "        'importance': dt.feature_importances_\n",
    "    })\n",
    "\n",
    "importance_df.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only plot importances > 0.01\n",
    "importance_df1 = importance_df[importance_df['importance'] >= 0.01]\n",
    "importance_df1.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "# (n_estimators=500, oob_score=True\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=10)\n",
    "print \"Mean R^2 = {:.3}\".format(scores.mean())\n",
    "\n",
    "# Print Classification Summary Scored in Development \n",
    "print \"Accuracy Score on Dev Set: %0.2f%%\" % (rf.score(X_dev,y_dev))\n",
    "preds = rf.predict(X_dev)\n",
    "mse = np.mean((preds - y_dev) ** 2)\n",
    "print 'Mean squared error = {}'.format(mse)\n",
    "print 'R^2 = {}\\n'.format(rf.score(X_train, y_train))\n",
    "\n",
    "# print metrics.classification_report(y_dev, preds) # target_names = target_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Feature Importnace in RF \n",
    "importance_df = pd.DataFrame({\n",
    "        'feature': predictors,\n",
    "        'importance': rf.feature_importances_\n",
    "    })\n",
    "\n",
    "importance_df.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only plot importances > 0.01\n",
    "importance_df1 = importance_df[importance_df['importance'] >= 0.01]\n",
    "importance_df1.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
