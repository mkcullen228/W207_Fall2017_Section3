{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Cover Type Prediction\n",
    "### Authors: Maura Cullin, Mike Gruzynski\n",
    "#### w207 Final project section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: We choose to do the kaggle competition: Forest Cover Type Prediction (https://www.kaggle.com/c/forest-cover-type-prediction). The data comes from US Forest Service (USFS) Region 2 Resource Information System data and the Independent variables were then derived from data obtained from the US Geological Survey and USFS.\n",
    "\n",
    "This study area includes four wilderness areas located in the Roosevelt National Forest, which is located in northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes (or natural setting) rather than forest management practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mauracullen/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import gridspec\n",
    "import sys\n",
    "import os.path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# For producing decision tree diagrams.\n",
    "from IPython.core.display import Image, display\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File /Users/mauracullen/Documents/UCB_MIDS/W207_AppliedMachineLearning/W207_Fall2017_Section3/Final_Project/final_project/train.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c23ad2491c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the raw dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/final_project\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cover_Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cover_Type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mauracullen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mauracullen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mauracullen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mauracullen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mauracullen/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File /Users/mauracullen/Documents/UCB_MIDS/W207_AppliedMachineLearning/W207_Fall2017_Section3/Final_Project/final_project/train.csv does not exist"
     ]
    }
   ],
   "source": [
    "# load the raw dataset\n",
    "file_path = os.getcwd() + \"/final_project\"\n",
    "file_path = os.getcwd() + \"/data\"\n",
    "train_df = pd.read_csv(file_path + '/train.csv')\n",
    "target = train_df['Cover_Type']\n",
    "train_df = train_df.drop('Cover_Type', 1)\n",
    "train_df = train_df.drop('Soil_Type7', 1)\n",
    "train_df = train_df.drop('Soil_Type15', 1)\n",
    "train_df = train_df.drop('Id',1)\n",
    "\n",
    "# print train_df.head()\n",
    "test_df = pd.read_csv(file_path + '/test.csv')\n",
    "\n",
    "predictors = train_df.columns\n",
    "l = len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 15120 rows in the dataframe, with no missing data. The data is clean and the only filter that will be performed on the dataset is on \"Soil_Type7\",\"Soil_Type15\" because there is only values of 0 for all rows in the training data. We will remove the two columns from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up variable types\n",
    "continuous = [\n",
    "            'Elevation', 'Aspect', 'Slope',\n",
    "            'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "            'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
    "            'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points'\n",
    "            ]\n",
    "\n",
    "# remove \"Soil_Type7\",\"Soil_Type15\" because their data was only one value (data cannot be seperable and irrelevantc)\n",
    "binary = [\n",
    "        'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
    "        'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n",
    "        'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n",
    "        'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11',\n",
    "        'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n",
    "        'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19',\n",
    "        'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23',\n",
    "        'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
    "        'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31',\n",
    "        'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35',\n",
    "        'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39',\n",
    "        'Soil_Type40'\n",
    "        ]\n",
    "\n",
    "predictors = continuous + binary\n",
    "\n",
    "# objective variable is a category\n",
    "target_str = 'Cover_Type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = train_df.columns\n",
    "\n",
    "# Prepare Test Set \n",
    "test_df = test_df.drop('Soil_Type7', 1)\n",
    "test_df = test_df.drop('Soil_Type15', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Distribution\n",
    "target_names_list = [\"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \n",
    "                \"Cottonwood/Willow\", \"Aspen\", \"Douglas-fir\", \"Krummholz\"]\n",
    "target_names_dict = {1:\"Spruce/Fir\",2:\"Lodgepole Pine\",3:\"Ponderosa Pine\", \n",
    "                4:\"Cottonwood/Willow\",5:\"Aspen\", 6:\"Douglas-fir\", 7:\"Krummholz\"}\n",
    "\n",
    "counts = target.value_counts(sort=False)\n",
    "total = sum(counts)\n",
    "print \"Class Distribution in Target Var: \"\n",
    "print \"\\tClass                    |  Number of Examples\"\n",
    "for c in np.unique(target):\n",
    "    print \"\\t%i, %-17s     |   %i (%0.2f%%)\" %(c, target_names_dict[c] ,counts[c], (counts[c]/float(total))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix on continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df[continuous].corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "corr_columns = list(corr.columns)\n",
    "corr_comparison_list = []\n",
    "corr_value_list = []\n",
    "for column in corr:\n",
    "    temp_corr_columns = corr_columns   \n",
    "    temp_corr_columns.remove(column)\n",
    "    for index in corr.loc[temp_corr_columns, column].index:\n",
    "        corr_comparison_list.append(str(column) + '_vs_' + str(index))\n",
    "        corr_value_list.append(corr.loc[index, column])\n",
    "        \n",
    "corr_df = pd.DataFrame({\n",
    "    \"corr_comparison\" : corr_comparison_list,\n",
    "    \"corr_value\" : corr_value_list \n",
    "})\n",
    "\n",
    "corr_df = corr_df.iloc[corr_df.corr_value.abs().argsort()][::-1].reset_index()\n",
    "corr_df = corr_df.loc[:, ~corr_df.columns.str.contains('index')]\n",
    "print corr_df.loc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above correlation matrix, we see combination of variables that may cause collinearity issues in the analysis.\n",
    "\n",
    "For example Hillshade_3pm is correlated heavily with Hillshade_9am (corr ~ -0.77).\n",
    "\n",
    "The above dataframe output shows the top 10 (absolute value) correlation values between continuous variables, so we will need to be knowledgeable about having both of the features in the analysis in the top 10 list. The duplicated information may overfit the data and reduce the generalizability of the final analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize continuous variables mapped to 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 120))\n",
    "gs = gridspec.GridSpec(23, 2) \n",
    "df_column_list = list(train_df[continuous].columns)\n",
    "itr = 0\n",
    "cmap = mpl.colors.ListedColormap(['black','red', 'green', 'blue', 'cyan', 'violet', 'yellow'])\n",
    "black_patch = mpatches.Patch(color='black', label='1')\n",
    "red_patch = mpatches.Patch(color='red', label='2')\n",
    "green_patch = mpatches.Patch(color='green', label='3')\n",
    "blue_patch = mpatches.Patch(color='blue', label='4')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='5')\n",
    "violet_patch = mpatches.Patch(color='violet', label='6')\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='7')\n",
    "\n",
    "\n",
    "for feature in corr:    \n",
    "    temp_columns = df_column_list\n",
    "    index_value = temp_columns.index(feature) + 1\n",
    "    \n",
    "    for sub_feature in temp_columns[index_value:]:\n",
    "        ax = plt.subplot(gs[itr])\n",
    "        ax.scatter(train_df.loc[:, feature], train_df.loc[:, sub_feature], \n",
    "                   c=target, alpha=0.3, cmap=cmap)\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel(sub_feature)\n",
    "        ax.set_title('{} vs {}'.format(feature, sub_feature, fontsize=12))\n",
    "        ax.legend(handles=[black_patch, red_patch, green_patch, blue_patch, cyan_patch, violet_patch, yellow_patch])\n",
    "        itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing/bad data in Hillshade 3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_parameter_grid = {'n_estimators': [300, 500, 600, 700, 800]}\n",
    "\n",
    "\n",
    "rf_3pm_fix_df = train_df[continuous]\n",
    "rf_3pm_fix_df = rf_3pm_fix_df.loc[rf_3pm_fix_df['Hillshade_3pm'] != 0.0]\n",
    "rf_3pm_fix_df_target = rf_3pm_fix_df['Hillshade_3pm']\n",
    "rf_3pm_fix_df = rf_3pm_fix_df[['Elevation',\n",
    " 'Aspect',\n",
    " 'Slope',\n",
    " 'Horizontal_Distance_To_Hydrology',\n",
    " 'Vertical_Distance_To_Hydrology',\n",
    " 'Horizontal_Distance_To_Roadways',\n",
    " 'Hillshade_9am',\n",
    " 'Hillshade_Noon',\n",
    " 'Horizontal_Distance_To_Fire_Points']]\n",
    "\n",
    "param_searcher = GridSearchCV(RandomForestRegressor(), RF_parameter_grid, cv=5)\n",
    "X_train_3pm, X_dev_3pm, y_train_3pm, y_dev_3pm = train_test_split(rf_3pm_fix_df, rf_3pm_fix_df_target)\n",
    "param_searcher.fit(X_train_3pm, y_train_3pm)\n",
    "\n",
    "model_best_3pm = RandomForestRegressor(**param_searcher.best_params_)\n",
    "model_best_3pm.fit(X_train_3pm, y_train_3pm)\n",
    "pred_3pm = model_best_3pm.predict(X_dev_3pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_3pm_percentage = (pred_3pm/y_dev_3pm - 1)*100\n",
    "difference_3pm_percentage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good fit, average difference is 0.04% off with the majority of data (1 and 3rd quartile) in between ~ +/- 0.35 % difference between predicted Hillshade at 3pm and Actual Hillshade at 3pm. Will move forward and replace zero values with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 0\n",
    "while itr < len(train_df):\n",
    "    if train_df.loc[itr, 'Hillshade_3pm'] == 0:\n",
    "        train_df.loc[itr, 'Hillshade_3pm'] = model_best_3pm.predict(train_df.loc[itr, ['Elevation', 'Aspect', 'Slope', \n",
    "                        'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
    "                       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', \n",
    "                       'Horizontal_Distance_To_Fire_Points']])\n",
    "        \n",
    "    itr += 1\n",
    "print train_df.Hillshade_3pm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more zero values for HillShade at 3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_df.loc[:, 'Hillshade_Noon'], train_df.loc[:, 'Hillshade_3pm'], \n",
    "           c=target, alpha=0.3, cmap=cmap)\n",
    "plt.xlabel('Hillshade_Noon')\n",
    "plt.ylabel('Hillshade_3pm')\n",
    "plt.title('{} vs {}'.format('Hillshade_Noon', 'Hillshade_3pm', fontsize=12))\n",
    "plt.legend(handles=[black_patch, red_patch, green_patch, blue_patch, cyan_patch, violet_patch, yellow_patch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness information on variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[continuous].skew().sort_values()\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(4, 3) \n",
    "\n",
    "itr = 0\n",
    "for feature in train_df[continuous]:   \n",
    "    if itr == 9:\n",
    "        itr += 1\n",
    "    ax = plt.subplot(gs[itr])\n",
    "    ax.hist(train_df.loc[:, feature])\n",
    "    ax.set_xlabel(\"Histogram Bin\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title('Feature={}, Skew={}'.format(feature, round(train_df.loc[:, feature].skew(), 3), fontsize=12))\n",
    "    \n",
    "    itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above histograms (with skewness value in the title of each plot), we can see that there are some high skews with potential of some log transformations if the math allows it (i.e cant do a log transformations on a zero) so if the column data has a zero we will have to think of doing something else if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After looking at the scatter plots of 2D projection, we can see that there are some very distance regions for target delineation of target cases (the first nine plots - titles shown below):\n",
    "\n",
    "Elevation vs all of its other covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots of continuous variables by Cover_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxplot_df_cont = train_df[:]\n",
    "boxplot_df_cont['Cover_Type'] = target\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(20, 20))\n",
    "\n",
    "itr_1 = 0\n",
    "itr_2 = 0\n",
    "for feature in continuous:\n",
    "    if itr_2 == 3:\n",
    "        itr_2 = 0\n",
    "        itr_1 += 1\n",
    "    if itr_1 == 3:\n",
    "        itr_2 = 1\n",
    "    temp_list = []\n",
    "    temp_list.append(feature)\n",
    "    temp_list.append(u'Cover_Type')\n",
    "    boxplot_df_cont[temp_list].boxplot(by='Cover_Type', ax=axes[itr_1, itr_2])\n",
    "    itr_2 += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above boxplots by forest cover type, the elevation shows very distinct values per category. Elevation is looking like it is a very important feature in determining the target value of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary variable exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = train_df[binary]\n",
    "df_binary['Cover_Type'] = target\n",
    "fig, axes = plt.subplots(nrows=21, ncols=2, figsize=(20, 60))\n",
    "\n",
    "df_binary_columns = []\n",
    "df_dict = {}\n",
    "itr = 0\n",
    "master_df = pd.DataFrame()\n",
    "pic_itr1 = 0\n",
    "pic_itr2 = 0\n",
    "for feature in df_binary:\n",
    "    if 'Cover_Type' not in feature:\n",
    "        df_binary_columns.append(feature)\n",
    "        temp_list = []\n",
    "        temp_list.append(feature)\n",
    "        temp_list.append(u'Cover_Type')\n",
    "        \n",
    "        if pic_itr2 == 2:\n",
    "            pic_itr2 = 0\n",
    "            pic_itr1 += 1\n",
    "        \n",
    "        itr = 1\n",
    "        for name, group in df_binary.loc[:, temp_list].groupby('Cover_Type'):\n",
    "            if itr == 1:\n",
    "                df_out = group[feature].value_counts()\n",
    "                df_out.name = 'type_{}'.format(itr)\n",
    "            else:\n",
    "                df_temp = group[feature].value_counts()\n",
    "                df_temp.name = 'type_{}'.format(itr)\n",
    "                df_out = pd.concat([df_out, df_temp], axis = 1)\n",
    "            itr += 1\n",
    "            \n",
    "        \n",
    "        print df_out.plot(kind='bar', title =feature, \n",
    "                          color=['black','red', 'green', 'blue', 'cyan', 'violet', 'yellow'],\n",
    "                          ax=axes[pic_itr1, pic_itr2])\n",
    "        master_df = master_df.append(df_out.rename({0: feature + '_0', 1: feature + '_1'}))\n",
    "        \n",
    "        pic_itr2 += 1\n",
    "        \n",
    "# print master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that a lot of the soil type information is very similar with not a lot of added information to the data analysis. This information along with the continuous data information will help make engineering feature elimination decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization study on variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(train_df[predictors], target)\n",
    "\n",
    "ridge = RidgeCV(alphas= np.linspace(5, 50, 100))\n",
    "ridge.fit(train_df[predictors], target)\n",
    "\n",
    "lasso = LassoCV(alphas= 2. ** np.arange(-10, 10))\n",
    "lasso.fit(train_df[predictors], target)\n",
    "\n",
    "en = ElasticNetCV(l1_ratio=np.linspace(.05, .95, 20), alphas= 2. ** np.arange(-10, 10))\n",
    "en.fit(train_df[predictors], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Train R-squared: {:.3}'.format(linreg.score(train_df[predictors], target))\n",
    "print 'Ridge Train R-squared: {:.3}'.format(ridge.score(train_df[predictors], target))\n",
    "print 'Lasso Train R-squared: {:.3}'.format(lasso.score(train_df[predictors], target))\n",
    "print 'EN Train R-squared: {:.3}'.format(en.score(train_df[predictors], target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.DataFrame({\n",
    "        'variable': predictors,\n",
    "        'OLS': linreg.coef_,\n",
    "        'Ridge': ridge.coef_,\n",
    "        'Lasso': lasso.coef_,\n",
    "        'ElasticNet': en.coef_\n",
    "    })\n",
    "\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above regression equations, we see that they are not good for fit prediction. However, we also see that some variables are poor added information for prediction:\n",
    "\n",
    "Soil_Type1\n",
    "Soil_Type8\n",
    "Soil_Type9\n",
    "Soil_Type11\n",
    "Soil_Type25\n",
    "Soil_Type27\n",
    "Soil_Type28\n",
    "Soil_Type34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(train_df, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Print Classification Summary Scored in Development \n",
    "print \"Accuracy Score on Dev Set: %0.2f%%\" % (dt.score(X_dev,y_dev))\n",
    "preds = dt.predict(X_dev)\n",
    "mse = np.mean((preds - y_dev) ** 2)\n",
    "print 'Mean squared error = {}'.format(mse)\n",
    "print 'R^2 = {}\\n'.format(dt.score(X_train, y_train))\n",
    "\n",
    "print metrics.classification_report(y_dev, preds) # target_names = target_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report Feature Importnace in DTREE \n",
    "importance_df = pd.DataFrame({\n",
    "        'feature': predictors,\n",
    "        'importance': dt.feature_importances_\n",
    "    })\n",
    "\n",
    "importance_df.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# only plot importances > 0.01\n",
    "importance_df1 = importance_df[importance_df['importance'] >= 0.01]\n",
    "importance_df1.sort_values('importance').plot(x='feature', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the decision tree, we also do not see any of the variables that Lasso and Ridge eliminated in the important variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_transform(aspect):\n",
    "    if aspect >= 0 and aspect < 45:\n",
    "        # N-NE\n",
    "        return 1\n",
    "    elif aspect >= 45 and aspect < 90:\n",
    "        # N-NE\n",
    "        return 2\n",
    "    elif aspect >= 90 and aspect < 135:\n",
    "        # N-NE\n",
    "        return 3\n",
    "    elif aspect >= 135 and aspect < 180:\n",
    "        # N-NE\n",
    "        return 4\n",
    "    elif aspect >= 180 and aspect < 225:\n",
    "        # N-NE\n",
    "        return 5\n",
    "    elif aspect >= 225 and aspect < 270:\n",
    "        # N-NE\n",
    "        return 6\n",
    "    elif aspect >= 270 and aspect < 315:\n",
    "        # N-NE\n",
    "        return 7\n",
    "    elif aspect >= 315 and aspect < 360:\n",
    "        # N-NE\n",
    "        return 8\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def binary_hydrology(hydrology):\n",
    "    if hydrology >= 0:\n",
    "        # N-NE\n",
    "        return 1\n",
    "    elif hydrology < 0:\n",
    "        # N-NE\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_scaler = MinMaxScaler()\n",
    "X_train_normalize_continuous = normalize_scaler.fit_transform(X_train.loc[:, continuous])\n",
    "X_dev_normalize_continuous = normalize_scaler.transform(X_dev.loc[:, continuous])\n",
    "\n",
    "demean_train = X_train.loc[:, continuous]\n",
    "demean_train = (demean_train - demean_train.mean())/demean_train.std()\n",
    "demean_test = X_dev.loc[:, continuous]\n",
    "demean_test = (demean_test - demean_test.mean())/demean_test.std()\n",
    "\n",
    "squared_train = X_train.loc[:, continuous]\n",
    "squared_df_train = np.square(squared_train)\n",
    "squared_test = X_dev.loc[:, continuous]\n",
    "squared_df_test = np.square(squared_test)\n",
    "\n",
    "continuous_custom = list(continuous)\n",
    "continuous_custom.append(\"Aspect2\")\n",
    "continuous_custom.append(\"Aspect3\")\n",
    "continuous_custom.append(\"Verticle_Hydro_Binary\")\n",
    "continuous_custom.append(\"Hydro_rms\")\n",
    "continuous_custom.append(\"Elev_vs_vHydro\")\n",
    "continuous_custom.append(\"Elev_vs_hHydro\")\n",
    "continuous_custom.append(\"Elev_vs_hRoad\")\n",
    "continuous_custom.append(\"Elev_vs_hFire\")\n",
    "continuous_custom.append(\"Hydro_vs_Fire_add\")\n",
    "continuous_custom.append(\"Hydro_vs_Fire_sub\")\n",
    "continuous_custom.append(\"Hydro_vs_Road_add\")\n",
    "continuous_custom.append(\"Hydro_vs_Road_sub\")\n",
    "continuous_custom.append(\"Road_vs_Fire_add\")\n",
    "continuous_custom.append(\"Road_vs_Fire_sub\")\n",
    "\n",
    "custom_transformation_train = train_df.loc[:, continuous]\n",
    "custom_transformation_train['Aspect2'] = np.power(custom_transformation_train['Aspect'], 1./2)\n",
    "custom_transformation_train['Aspect3'] = custom_transformation_train.Aspect.map(aspect_transform)\n",
    "custom_transformation_train['Verticle_Hydro_Binary'] = custom_transformation_train.Vertical_Distance_To_Hydrology.map(binary_hydrology)\n",
    "custom_transformation_train['Hydro_rms'] = (custom_transformation_train['Horizontal_Distance_To_Hydrology']**2+custom_transformation_train['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "custom_transformation_train['Elev_vs_vHydro'] = custom_transformation_train.Elevation - custom_transformation_train.Vertical_Distance_To_Hydrology\n",
    "custom_transformation_train['Elev_vs_hHydro'] = custom_transformation_train.Elevation - custom_transformation_train.Horizontal_Distance_To_Hydrology\n",
    "custom_transformation_train['Elev_vs_hRoad'] = custom_transformation_train.Elevation - custom_transformation_train.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_train['Elev_vs_hFire'] = custom_transformation_train.Elevation - custom_transformation_train.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_train['Hydro_vs_Fire_add'] = custom_transformation_train.Horizontal_Distance_To_Hydrology - custom_transformation_train.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_train['Hydro_vs_Fire_sub'] = custom_transformation_train.Horizontal_Distance_To_Hydrology - custom_transformation_train.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_train['Hydro_vs_Road_add'] = custom_transformation_train.Horizontal_Distance_To_Hydrology - custom_transformation_train.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_train['Hydro_vs_Road_sub'] = custom_transformation_train.Horizontal_Distance_To_Hydrology - custom_transformation_train.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_train['Road_vs_Fire_add'] = custom_transformation_train.Horizontal_Distance_To_Roadways - custom_transformation_train.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_train['Road_vs_Fire_sub'] = custom_transformation_train.Horizontal_Distance_To_Roadways - custom_transformation_train.Horizontal_Distance_To_Fire_Points\n",
    "\n",
    "custom_transformation_test = test_df.loc[:, continuous]\n",
    "custom_transformation_test['Aspect2'] = np.power(custom_transformation_test['Aspect'], 1./2)\n",
    "custom_transformation_test['Aspect3'] = custom_transformation_test.Aspect.map(aspect_transform)\n",
    "custom_transformation_test['Verticle_Hydro_Binary'] = custom_transformation_test.Vertical_Distance_To_Hydrology.map(binary_hydrology)\n",
    "custom_transformation_test['Hydro_rms'] = (custom_transformation_test['Horizontal_Distance_To_Hydrology']**2+custom_transformation_test['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "custom_transformation_test['Elev_vs_vHydro'] = custom_transformation_test.Elevation - custom_transformation_test.Vertical_Distance_To_Hydrology\n",
    "custom_transformation_test['Elev_vs_hHydro'] = custom_transformation_test.Elevation - custom_transformation_test.Horizontal_Distance_To_Hydrology\n",
    "custom_transformation_test['Elev_vs_hRoad'] = custom_transformation_test.Elevation - custom_transformation_test.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_test['Elev_vs_hFire'] = custom_transformation_test.Elevation - custom_transformation_test.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_test['Hydro_vs_Fire_add'] = custom_transformation_test.Horizontal_Distance_To_Hydrology - custom_transformation_test.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_test['Hydro_vs_Fire_sub'] = custom_transformation_test.Horizontal_Distance_To_Hydrology - custom_transformation_test.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_test['Hydro_vs_Road_add'] = custom_transformation_test.Horizontal_Distance_To_Hydrology - custom_transformation_test.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_test['Hydro_vs_Road_sub'] = custom_transformation_test.Horizontal_Distance_To_Hydrology - custom_transformation_test.Horizontal_Distance_To_Roadways\n",
    "custom_transformation_test['Road_vs_Fire_add'] = custom_transformation_test.Horizontal_Distance_To_Roadways - custom_transformation_test.Horizontal_Distance_To_Fire_Points\n",
    "custom_transformation_test['Road_vs_Fire_sub'] = custom_transformation_test.Horizontal_Distance_To_Roadways - custom_transformation_test.Horizontal_Distance_To_Fire_Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cust, X_dev_cust, y_train_cust, y_dev_cust = train_test_split(custom_transformation_train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "gs = gridspec.GridSpec(5, 5) \n",
    "\n",
    "itr = 0\n",
    "for feature in custom_transformation_train[continuous_custom]:   \n",
    "    ax = plt.subplot(gs[itr])\n",
    "    ax.hist(custom_transformation_train.loc[:, feature])\n",
    "    ax.set_xlabel(\"Histogram Bin\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title('Feature={}, Skew={}'.format(feature, round(custom_transformation_train.loc[:, feature].skew(), 3), fontsize=12))\n",
    "    \n",
    "    itr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.get_params().keys()\n",
    "def model_optimize(model, X_train, y_train, X_test, y_test, param_dict):\n",
    "    \n",
    "    df_param = pd.DataFrame()\n",
    "    \n",
    "    if len(param_dict) > 0:\n",
    "        param_searcher = GridSearchCV(model(), param_dict, cv=5)\n",
    "        param_searcher.fit(X_train, y_train)\n",
    "\n",
    "        df_param = pd.DataFrame(list(param_searcher.cv_results_['params']))\n",
    "        df_param['mean_test_score'] = param_searcher.cv_results_['mean_test_score']\n",
    "        df_param = df_param.sort_values('mean_test_score', ascending=False)\n",
    "        df_param = df_param.loc[:, ~df_param.columns.str.contains('index')]\n",
    "\n",
    "        model_best = model(**param_searcher.best_params_)\n",
    "    else:\n",
    "        model_best = model()\n",
    "        \n",
    "        \n",
    "    model_best.fit(X_train, y_train)\n",
    "    model_score = model_best.score(X_test, y_test)\n",
    "    pred = model_best.predict(X_test)\n",
    "    \n",
    "    df_confusion = pd.DataFrame(confusion_matrix(y_test, pred), \n",
    "                                columns=['Predicted=1', 'Predicted=2', 'Predicted=3', 'Predicted=4', 'Predicted=5', 'Predicted=6', 'Predicted=7'], \n",
    "                                index=['Actual=1', 'Actual=2', 'Actual=3', 'Actual=4', 'Actual=5', 'Actual=6', 'Actual=7']\n",
    "    )\n",
    "                  \n",
    "    return model_score, df_param, df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### kNN - k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_parameter_grid = {\n",
    "    'n_neighbors': [1, 2, 5, 7, 10, 15, 20, 25, 35, 45, 60, 80, 100, 150],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "kNN_model_score, kNN_df_param, kNN_df_confusion = model_optimize(KNeighborsClassifier, X_train, y_train, X_dev, y_dev, knn_parameter_grid)\n",
    "kNN_model_score_custom, kNN_df_param_custom, kNN_df_confusion_custom = model_optimize(KNeighborsClassifier, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, knn_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print kNN_model_score\n",
    "print kNN_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_parameter_grid = {}\n",
    "\n",
    "NB_model_score, NB_df_param, NB_df_confusion = model_optimize(GaussianNB, X_train, y_train, X_dev, y_dev, NB_parameter_grid)\n",
    "NB_model_score_custom, NB_df_param_custom, NB_df_confusion_custom = model_optimize(GaussianNB, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, NB_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print NB_model_score\n",
    "print NB_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                     'min_samples_leaf': [1, 2, 5, 10, 20],\n",
    "                     'max_features': [2, 3, 5, 10, 15, 30, 52], \n",
    "                     'max_depth': [1, 2, 3, 5, 10, 15, 20, 30, 40]}\n",
    "DT_parameter_filtered1 = {'criterion': ['gini', 'entropy'],\n",
    "                     'min_samples_leaf': [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80],\n",
    "                     'max_features': [2, 3, 5, 14], \n",
    "                     'max_depth': [1, 2, 3, 5, 10, 15, 20, 30, 40]}\n",
    "DT_parameter_filtered2 = {'criterion': ['gini', 'entropy'],\n",
    "                     'min_samples_leaf': [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80],\n",
    "                     'max_features': [2, 3, 5, 10], \n",
    "                     'max_depth': [1, 2, 3, 5, 10, 15, 20, 30, 40]}\n",
    "\n",
    "DT_model_score, DT_df_param, DT_df_confusion = model_optimize(DecisionTreeClassifier, X_train, y_train, X_dev, y_dev, DT_parameter_grid)\n",
    "DT_model_score_custom, DT_df_param_custom, DT_df_confusion_custom = model_optimize(DecisionTreeClassifier, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, DT_parameter_filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print DT_model_score\n",
    "print DT_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_parameter_grid = {'criterion': ['entropy'],\n",
    "                     'n_estimators': [300, 500, 600, 700, 800]}\n",
    "\n",
    "\n",
    "RF_model_score, RF_df_param, RF_df_confusion = model_optimize(RandomForestClassifier, X_train, y_train, X_dev, y_dev, RF_parameter_grid)\n",
    "RF_model_score_custom, RF_df_param_custom, RF_df_confusion_custom = model_optimize(RandomForestClassifier, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, RF_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print RF_model_score\n",
    "print RF_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used best n_estimator for each version of the model above to fill into AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB = AdaBoostClassifier(base_estimator=RandomForestClassifier(), n_estimators=600)\n",
    "AB_custom = AdaBoostClassifier(base_estimator=RandomForestClassifier(), n_estimators=800)\n",
    "\n",
    "AB.fit(X_train, y_train)\n",
    "AB_custom.fit(X_train_cust, y_train_cust)\n",
    "\n",
    "AB_model_score = AB.score(X_dev, y_dev)\n",
    "AB_model_score_custom = AB_custom.score(X_dev_cust, y_dev_cust)\n",
    "\n",
    "pred = AB.predict(X_dev)\n",
    "pred_custom = AB_custom.predict(custom_transformation_test)\n",
    "\n",
    "AB_df_confusion = pd.DataFrame(confusion_matrix(y_dev, pred), \n",
    "                            columns=['Predicted=1', 'Predicted=2', 'Predicted=3', 'Predicted=4', 'Predicted=5', 'Predicted=6', 'Predicted=7'], \n",
    "                            index=['Actual=1', 'Actual=2', 'Actual=3', 'Actual=4', 'Actual=5', 'Actual=6', 'Actual=7'])\n",
    "AB_df_confusion_custom = pd.DataFrame(confusion_matrix(y_dev, pred_custom), \n",
    "                            columns=['Predicted=1', 'Predicted=2', 'Predicted=3', 'Predicted=4', 'Predicted=5', 'Predicted=6', 'Predicted=7'], \n",
    "                            index=['Actual=1', 'Actual=2', 'Actual=3', 'Actual=4', 'Actual=5', 'Actual=6', 'Actual=7'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print AB_model_score\n",
    "print AB_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_parameter_grid = {\n",
    "    'n_estimators': [300, 400, 500, 600, 700]\n",
    "}\n",
    "\n",
    "GB_model_score, GB_df_param, GB_df_confusion = model_optimize(GradientBoostingClassifier, X_train, y_train, X_dev, y_dev, GB_parameter_grid)\n",
    "GB_model_score_custom, GB_df_param_custom, GB_df_confusion_custom = model_optimize(GradientBoostingClassifier, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, GB_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print GB_model_score\n",
    "print GB_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {'C': [1500, 2000, 2500], \n",
    "              'gamma': [0.5, 1.0, 1.5], \n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "SVM_model_score, SVM_df_param, SVM_df_confusion = model_optimize(SVC, X_train, y_train, X_dev, y_dev, param_grid_svm)\n",
    "SVM_model_score_custom, SVM_df_param_custom, SVM_df_confusion_custom = model_optimize(SVC, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, param_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print SVM_model_score\n",
    "print SVM_model_score_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XRF_parameter_grid = {'criterion': ['entropy'],\n",
    "                     'n_estimators': [300, 500, 600, 700, 800]}\n",
    "\n",
    "\n",
    "XRF_model_score, XRF_df_param, XRF_df_confusion = model_optimize(ExtraTreesClassifier, X_train, y_train, X_dev, y_dev, RF_parameter_grid)\n",
    "XRF_model_score_custom, XRF_df_param_custom, XRF_df_confusion_custom = model_optimize(ExtraTreesClassifier, X_train_cust, y_train_cust, X_dev_cust, y_dev_cust, RF_parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print XRF_model_score\n",
    "print XRF_model_score_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Aanlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_components(n_features, X_train_data, y_train_data, X_dev_data, y_dev_dat):\n",
    "    pca = PCA(n_components=n_features)\n",
    "    X_transformed = pca.fit_transform(X_train_data)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_transformed, y_train_data)\n",
    "    return lr.score(pca.transform(X_dev_data), y_dev_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analysis_reduction(train_data, test_data):\n",
    "    pca = PCA()\n",
    "    pca.fit(train_data)\n",
    "\n",
    "    # Choose number of Compenents to Keep\n",
    "    num_comp = np.sum(np.cumsum(pca.explained_variance_ratio_) < var_explained)\n",
    "    print \"Keep Number of Compenents: %i, explaining %0.2f%% Variance\" %(num_comp, var_explained)\n",
    "\n",
    "    # plot \n",
    "    plt.plot(pca.explained_variance_ratio_) \n",
    "    plt.title(\"PCA Explained Variance Ratio\")\n",
    "    plt.xlabel(\"Number of Fatures\")\n",
    "    plt.ylabel(\"Variance Ratio\")\n",
    "    plt.show()\n",
    "\n",
    "    # Reduce Dataset \n",
    "    pca = PCA(n_components=num_comp).fit(train_data)\n",
    "    X_train_pca = PCA(n_components=num_comp).fit_transform(train_data)\n",
    "    X_dev_pca = pca.transform(test_data)\n",
    "    print \"Reduced Train set shape: %i X %i\" % (X_train_pca.shape[0],X_train_pca.shape[1])\n",
    "    return (X_train_pca, X_dev_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat PCA datasets for training\n",
    "var_explained = 0.95\n",
    "\n",
    "# Standard Train Data \n",
    "out = [best_components(i, X_train, y_train, X_dev, y_dev) for i in range(1, X_train.shape[1])]\n",
    "plt.plot(out)\n",
    "plt.title(\"Standard Training Data\")\n",
    "plt.show()\n",
    "\n",
    "# Filtered Train Data\n",
    "out = [best_components(i, X_train_filtered, y_train_filtered, X_dev_filtered, y_dev_filtered) for i in range(1, X_train_filtered.shape[1])]\n",
    "plt.plot(out)\n",
    "plt.title(\"Filtered Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preform PCA on regular data and on\n",
    "(X_train_pca, X_dev_pca) = pca_analysis_reduction(X_train, X_dev)\n",
    "(X_train_filtered_pca, X_dev_filtered_pca) = pca_analysis_reduction(X_train_filtered, X_dev_filtered)\n",
    "print X_train_pca.shape, X_dev_pca.shape\n",
    "print X_train_filtered_pca.shape, X_dev_filtered_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.get_params().keys()\n",
    "def model_optimize(model, X_train, y_train, X_test, y_test, param_dict, return_model = False):\n",
    "    \n",
    "    df_param = pd.DataFrame()\n",
    "    \n",
    "    if len(param_dict) > 0:\n",
    "        param_searcher = GridSearchCV(model(), param_dict, cv=5)\n",
    "        param_searcher.fit(X_train, y_train)\n",
    "\n",
    "        df_param = pd.DataFrame(list(param_searcher.cv_results_['params']))\n",
    "        df_param['mean_test_score'] = param_searcher.cv_results_['mean_test_score']\n",
    "        df_param = df_param.sort_values('mean_test_score', ascending=False)\n",
    "        df_param = df_param.loc[:, ~df_param.columns.str.contains('index')]\n",
    "\n",
    "        model_best = model(**param_searcher.best_params_)\n",
    "    else:\n",
    "        model_best = model()\n",
    "        \n",
    "        \n",
    "    model_best.fit(X_train, y_train)\n",
    "    model_score = model_best.score(X_test, y_test)\n",
    "    pred = model_best.predict(X_test)\n",
    "    \n",
    "    df_confusion = pd.DataFrame(confusion_matrix(y_test, pred), \n",
    "                                columns=['Predicted=1', 'Predicted=2', 'Predicted=3', 'Predicted=4', 'Predicted=5', 'Predicted=6', 'Predicted=7'], \n",
    "                                index=['Actual=1', 'Actual=2', 'Actual=3', 'Actual=4', 'Actual=5', 'Actual=6', 'Actual=7']\n",
    "    )\n",
    "     \n",
    "    if return_model == False: \n",
    "        return model_score, df_param, df_confusion\n",
    "    else: \n",
    "        return model_score, df_param, df_confusion, model_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
